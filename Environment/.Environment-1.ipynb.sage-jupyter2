{"backend_state":"init","kernel":"python3","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"nbgrader":{"__altered":false,"__hash":502143487,"_root":{"entries":[["size",6],["_root",{"entries":[["size",6],["_root",{"entries":[["size",5],["_root",{"entries":[["size",1],["_root",{"entries":[["cocalc_minimal_stubs",false]],"ownerID":{}}],["__hash",-335098374],["__altered",false],["cocalc_minimal_stubs",false]],"ownerID":{}}],["__ownerID",null],["__hash",1043241747],["__altered",false],["cocalc_minimal_stubs",false]],"ownerID":{}}],["__ownerID",null],["__hash",-992900273],["__altered",false],["cocalc_minimal_stubs",false]],"ownerID":{}}],["__ownerID",null],["__hash",497048546],["__altered",false],["cocalc_minimal_stubs",false]],"ownerID":{}},"cocalc_minimal_stubs":false,"size":6}},"trust":false,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"36f8ab","input":"","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"bc6bb7","input":"","pos":13,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c6cd4e","input":"# provided code, no need to memorise\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef fit_polynomial_model(xs, ys, order=1):\n    model = Pipeline([('poly', PolynomialFeatures(degree=order)), ('linear', LinearRegression())])\n    model.fit(xs.reshape(-1,1), ys)\n    return model\n\ndef get_predictions(model, xs):\n    predictions = model.predict(xs.reshape(-1,1))\n    return predictions","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":false,"grade_id":"79acb4","locked":true,"schema_version":3,"solution":false,"task":false}},"pos":37,"type":"cell"}
{"cell_type":"code","exec_count":103,"id":"d6a02e","input":"from scipy.stats import linregress\nfreo_data=get_clean_data(FREO_DATA)\ndates=freo_data[:,DATE]\nsealevel=freo_data[:,MSL]\n\nlrg=linregress(x=get_clean_data(FREO_DATA),alternative='two-sided')\nlrg_line= [lrg[0]*i +lrg[1] for i in dates]\nprint(len(dates)/2)\nindex_midpoint=(np.ceil(len(dates)/2).astype(int))\nmid_date=dates[index_midpoint:]\nmid_sealevel=sealevel[index_midpoint:]\n\nlrg_mid=linregress(freo_data[index_midpoint:,:],alternative='two-sided')\nlrg_line_mid= [lrg_mid[0]*i +lrg_mid[1] for i in mid_date]\n\n\nplt.scatter(dates,sealevel)\nplt.plot(dates,lrg_line,color='yellow',linewidth=4)\nplt.plot(mid_date,lrg_line_mid,color='black',linewidth=4)\nplt.plot()","output":{"0":{"name":"stdout","output_type":"stream","text":"669.5\n"},"1":{"data":{"text/plain":"[]"},"exec_count":103,"output_type":"execute_result"},"2":{"data":{"image/png":"f2f846256a3ec2468436e313162ab3ff82ecfa7a","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":103,"metadata":{"image/png":{"height":411,"width":716},"needs_background":"light"},"output_type":"execute_result"}},"pos":22,"type":"cell"}
{"cell_type":"code","exec_count":159,"id":"7e5042","input":"def piecewise_linear(data, segments, quiet=False):\n    seg_date=[]\n    slope=[]\n    intercept=[]\n\n    date=data[:,DATE]\n    sealevel=data[:,MSL]\n    segmented_data=np.array_split(data,segments)\n\n    for each_seg in segmented_data:\n        #get linerreg of each segment\n        newslope=linregress(each_seg)[0]\n        newintercept=linregress(each_seg)[1]\n        slope.append(newslope)\n        intercept.append(newintercept)\n        if not quiet: #quite is false\n            plt.plot(each_seg[:,DATE],newslope*each_seg[:,DATE]+newintercept,color=\"black\",linewidth=4)\n\n    if not quiet:\n        lg_data=linregress(data,alternative='two-sided')\n        lg_full=([lg_data[0]*i +lg_data[1] for i in date])\n        plt.scatter(date,sealevel,alpha=0.5)\n        plt.plot(date,lg_full,color='red',linewidth=4)\n\n    slopestack=np.vstack(np.array(slope))\n    interstack=np.vstack(np.array(intercept))\n\n    return np.hstack((slopestack,interstack))\n\n","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"1906d691bcc452af3b428b914973e023","grade":false,"grade_id":"02075f","locked":false,"schema_version":3,"solution":true,"task":false}},"pos":33,"type":"cell"}
{"cell_type":"code","exec_count":160,"id":"498ac2","input":"freo_data = get_clean_data(FREO_DATA)\nparameters = piecewise_linear(freo_data, 10, True)\nassert_true(np.isclose(parameters[3,:], np.array([ 8.00435815e+00, -8.87899164e+03])).all())\nassert_true(np.isclose(parameters[:,0], np.array([ 1.92811184,  8.25061015,  0.94959073,  8.00435815,  1.38211083,\n       -0.48934848, -1.22357932,  5.51171932,  1.36488971, -6.27671185])).all())\nprint(\"So far, so good. Please continue with your own testing.\")\n","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"piecewise_linear","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"output":{"0":{"name":"stdout","output_type":"stream","text":"So far, so good. Please continue with your own testing.\n"}},"pos":34,"type":"cell"}
{"cell_type":"code","exec_count":161,"id":"86df26","input":"def predicted_level(historical_data, datetime):\n    regression=linregress(historical_data)\n    predicted=(regression[0]*datetime + regression[1])/1000\n    predicted_value=np.round(predicted,2)\n    return predicted_value\n","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"1b9d3764d1c80a71065cc9947f69b57c","grade":false,"grade_id":"4400bb","locked":false,"schema_version":3,"solution":true,"task":false}},"pos":25,"type":"cell"}
{"cell_type":"code","exec_count":162,"id":"689683","input":"freo_data = get_clean_data(FREO_DATA)\nassert_true(np.isclose(predicted_level(freo_data, 2020), 6.81))\nassert_true(np.isclose(predicted_level(freo_data, 2080), 6.91))\nprint(\"So far so good. You should do your own testing.\")\n","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"predicted_level","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"output":{"0":{"name":"stdout","output_type":"stream","text":"So far so good. You should do your own testing.\n"}},"pos":26,"type":"cell"}
{"cell_type":"code","exec_count":45,"id":"7520fe","input":"import numpy as np","pos":6,"type":"cell"}
{"cell_type":"code","exec_count":46,"id":"8f95f5","input":"data=np.loadtxt(FREO_DATA,delimiter=';',usecols=range(0,3))\nprint(data.dtype)","output":{"0":{"name":"stdout","output_type":"stream","text":"float64\n"}},"pos":7,"type":"cell"}
{"cell_type":"code","exec_count":47,"id":"ffc016","input":"import csv","pos":9,"type":"cell"}
{"cell_type":"code","exec_count":48,"id":"a8bae4","input":"data[0]","output":{"0":{"data":{"text/plain":"array([1897.0417, 6542.    ,    9.    ])"},"exec_count":48,"output_type":"execute_result"}},"pos":10,"type":"cell"}
{"cell_type":"code","exec_count":49,"id":"300143","input":"import numpy as np","pos":15,"type":"cell"}
{"cell_type":"code","exec_count":51,"id":"00a5f7","input":"from nose.tools import assert_equal, assert_true\nfreo_data = get_clean_data(FREO_DATA)\nassert_true(np.isclose(freo_data[0], np.array([1897.125, 6524])).all())\nprint(\"So far, so good. Please continue with your own testing.\")\n","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":true,"grade_id":"get_clean_data","locked":true,"points":1,"schema_version":3,"solution":false,"task":false}},"output":{"0":{"name":"stdout","output_type":"stream","text":"So far, so good. Please continue with your own testing.\n"}},"pos":17,"type":"cell"}
{"cell_type":"code","exec_count":52,"id":"c2cc21","input":"import matplotlib.pyplot as plt\nclean_data=get_clean_data(FREO_DATA)\nget_mean=np.mean(clean_data[:,1])\n#mean_arr=np.full(clean_data[:,1].size,get_mean)\nplt.scatter(clean_data[:,0],clean_data[:,1],alpha=0.4,label=\"fremantle sea level\")\n\n#clean_data[:,0 ] is the value for Date\nxs=np.linspace(np.min(clean_data[:,0]),np.max(clean_data[:,0]))\none_arr=np.ones(xs.size)\nline=one_arr*get_mean\nplt.plot(xs,line,color=\"yellow\",linewidth=4)\n#Add a horizontal line across the Axes.\n#plt.axhline(get_mean,c='yellow',linewidth=4,label='mean sea level')\nplt.legend()","output":{"0":{"data":{"text/plain":"<matplotlib.legend.Legend at 0x7f512225eb20>"},"exec_count":52,"output_type":"execute_result"},"1":{"data":{"image/png":"37696415afefd6c9982c68263aa7745afd774cbe","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":52,"metadata":{"image/png":{"height":411,"width":716},"needs_background":"light"},"output_type":"execute_result"}},"pos":20,"type":"cell"}
{"cell_type":"code","exec_count":54,"id":"19cabf","input":"def get_clean_data(station_data):\n    data=np.loadtxt(station_data,delimiter=';',usecols=range(0,3))\n    #mean sea level recorded as -99999 as false(remove missing month)\n    mask_msl=np.logical_not(data[:,MSL]== -99999)\n\n    #remove months with 6 or more missing day\n    no_missing_data=data[mask_msl] #store it after remove -9999 row\n    over6days=np.logical_not(no_missing_data[:,MISSING]>=6)\n    cleaned=(no_missing_data[over6days])\n    return cleaned[:,0:2]\n\n\n\nget_clean_data(FREO_DATA)","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"66241c692ec08c3a5288c5b7d0257dac","grade":false,"grade_id":"d1c0b0","locked":false,"schema_version":3,"solution":true,"task":false}},"output":{"0":{"data":{"text/plain":"array([[1897.125 , 6524.    ],\n       [1897.2083, 6557.    ],\n       [1897.2917, 6655.    ],\n       ...,\n       [2019.7917, 6700.    ],\n       [2019.875 , 6686.    ],\n       [2019.9583, 6777.    ]])"},"exec_count":54,"output_type":"execute_result"}},"pos":16,"type":"cell"}
{"cell_type":"code","exec_count":57,"id":"bcd9bc","input":"FREO_DATA = '111.rlrdata.txt'\n(DATE, MSL, MISSING) = (0, 1, 2)","metadata":{"deletable":false,"editable":false,"nbgrader":{"grade":false,"grade_id":"3eeeb9","locked":true,"schema_version":3,"solution":false,"task":false}},"pos":4,"type":"cell"}
{"cell_type":"code","exec_count":86,"id":"09c653","input":"plt.scatter(dates,sealevel)\nplt.plot(dates,lrg_line,color='yellow')\n\nlrg=linregress(x=get_clean_data(FREO_DATA),alternative='two-sided')\nlrg_line= [lrg[0]*i +lrg[1] for i in dates]","output":{"0":{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f510fc99700>]"},"exec_count":86,"output_type":"execute_result"},"1":{"data":{"image/png":"e07af6f8fef3f7965cae2c2d22fe9a81d84fa353","text/plain":"<Figure size 864x504 with 1 Axes>"},"exec_count":86,"metadata":{"image/png":{"height":411,"width":716},"needs_background":"light"},"output_type":"execute_result"}},"pos":27,"type":"cell"}
{"cell_type":"code","exec_count":98,"id":"9fbbd8","input":"split4=np.array_split(get_clean_data(FREO_DATA),4)\nlen(split4)","output":{"0":{"data":{"text/plain":"4"},"exec_count":98,"output_type":"execute_result"}},"pos":30,"type":"cell"}
{"cell_type":"markdown","id":"0897cb","input":"#### Higher-order models\n\n* Add a third-order polynomial model to your plot.\n\nIs this more what you might have anticipated? If so, why do you think the 3rd order model is better able to achieve it?","pos":40,"type":"cell"}
{"cell_type":"markdown","id":"08b5e8","input":"### Piecewise linear models\n\nExtending on the above idea, we will generate a piecewise linear model for the whole data. Unlike our previous piecewise linear models, our line segments will not have zero slope, but will be fitted to the data using linear regression.\n\n* Divide the data into quartiles (4 segments). Again plot the data points, and plot the linear model in red. Plot the four new segments in black.\n\nThis time, use `numpy.array_split()` to divide the data in four.","pos":29,"type":"cell"}
{"cell_type":"markdown","id":"0a44e2","input":"## Data Visualisation and Modelling","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"0beeb6","input":"#### Linear model\n\n* Using the two functions above, plot the data with a first-order polynomial model.\n\nBecause we'll be plotting a number of lines on the same graph, this time we'll use the default colour cycle. So that the first line is not the same colour as the scatter plot, use `plt.plot()` rather than `plt.scatter()` to produce the scatter plot. You can do this by setting the linestyle to None, and the marker to a circle:\n\n```\nplt.plot(my_xs, my_ys, 'o', ls=' ', alpha=0.5, label='Fremantle Sea Levels')\n```\n\nYour output should look like this:\n\n<div>\n<img src=\"firstorder.png\" width=\"600\">\n</div>\n","pos":38,"type":"cell"}
{"cell_type":"markdown","id":"21efa5","input":"We'll use another numpy convenience method to load the text, this time `np.loadtxt()`.\n\n* Read the first 3 columns of the sea level data into an array using `np.loadtxt()` (1 line of code). Check the array shape and data type are as you would expect.\n\n_Tip: You don't need to use array selection to remove the fourth column, check the API._\n\nBecause arrays are homogenous, numpy will attempt to find the *least general* data type to which it can cast *all* of the data. What should you expect in this case?\n\n* Print the `dtype` and see if you were right.\n\n","pos":5,"type":"cell"}
{"cell_type":"markdown","id":"23fa9f","input":"* Try adding a fouth-degree polynomial model to your plot.\n\nDoes the extra degree of freedom add any value? ","pos":41,"type":"cell"}
{"cell_type":"markdown","id":"2df5d3","input":"#### Q1. Putting it together [1 lab mark]\n\n* Write a function `get_clean_data(station_data)` that:\n  * reads in the data from the filename in `station_data`\n  * removes any missing months\n  * removes any months with 6 or more missing days\n  * returns an $n \\times 2$ array of floats containing the remaining dates and mean sea levels\n  \n\nYour function should use numpy arrays. It should not use lists or loops. You may assume the above assignments to the global constants DATE, MSL and MISSING (that is, they don't need to be passed into the function). (~ 6 lines)\n\nCheck your answers.\n\n","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"33d8b2","input":"#### Q2. Predicting the sea level [1 lab mark]\n\n* Write a method `predicted_level(historical_data, datetime)` that takes a set of historical data in the format returned by `get_clean_data()` and a \"`datetime`\" (see below) and returns the predicted sea level at that datetime in *metres*, rounded to 2 decimal places.\n\nFor this data we'll simply represent the 'datetime' as a float. The whole part of the float represents the year (a little like we did in the GDP case study). The fractional part represents a fraction of a year. So, for example, 2020 represents midnight on the last day of 2019, and 2020.5 represents the halfway point of 2020.\n\nFor our modelling we'll assume that the dates in `111.rlrdata` are expressed in this format. We know that this is not quite true and will introduce a small error, because the dates are expressed as the midpoints of months, assuming each month is one twelfth of a year (which is not quite accurate). So the representation of time used by The Permanent Service for Mean Sea Level is actually nonlinear! We could fix this, but it would distract from the aims of the lab.\n\n_Mini-challenge: What is the most it can be out by? (That is, the largest error in datetime this will introduce?)_\n\n---\n\n* What does your model predict the mean sea level will be in 2020? Add some grid lines to your plot to see if this rings true.\n* What does your model predict the mean sea level will be in 2050? 2100? 3000?\n","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"36646e","input":"### Linear Regression and `scipy.stats`\n\nIn the lectures we do linear regression from first principles. We also introduced the `scikit-learn` library. In this lab we'll explore some more libraries that are handy for future use.\n\nFor this section, we'll briefly introduce a very useful library - the [`scipy.stats` library](https://docs.scipy.org/doc/scipy/reference/stats.html) - another part of the \"Scientific Python\" collection.\n\n* Look for the heading *Correlation functions* and find the `linregress()` method. Have a look at its API.\n\nYou'll see that it performs a linear regression computation for you and returns a 5-tuple. We will make use of the `slope` and `intercept`, but the function also returns some other statistics relating to the confidence of the observed relationship between the x and y data.\n\n_Note: While we're on that page, notice in the \"See also:\" section, that there is another handy SciPy library, `scipy.optimize`, that will perform least squares optimisation (like we did in the GDP case study) on arbitrary functions._\n\n* Import `linregress` and use it to plot a linear model against the sea level data.\n\n_Tip: Use the form of `linregress` that doesn't require you to use the second ('y') parameter, in order to take the 2-D array directly._\n\nPlot the linear model in red with a line width of four, and an appropriate legend.\n\n","pos":21,"type":"cell"}
{"cell_type":"markdown","id":"3fc276","input":"<h3>Lab 9</h3>\n\n# The Environment Redux: Modelling Environmental Trends\n\n<div>\n   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   <img src=\"extreme-weather.jpg\" width=60%/>\n</div>\n<br>\n\n#### _Modelling Time!_\n\n_Time to enjoy the fruits of our labour over the semester!_\n\nIn our financial case study (GDP per Capita) we looked at ways of quantitatively identifying or *modelling* trends in data.\n\nWe can now apply these new tools along with all our \"numpy know-how\" to identify trends in some of our other case studies. In this lab we will focus again on the environment, starting with the sea level data.","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"4b0030","input":"The above model is quite disjoint. An alternative is to join the midpoints of each line segment. Or alternatively, join the means of each data segment.\n\n* On a new version of your function from above, plot an additional line by:\n  * finding the median of the dates in each segment of data (xs)\n  * finding the mean of the sample points (sea levels) in each segment of data (ys)\n  * plotting a line of the ys against the xs (in a new colour)\n\nYou should have a single piecewise-linear line joining up the individual segments.\n\nTry this for different numbers of segments, ranging from about 3 to 100. What do you notice as you get to higher numbers of segments? (How does this relate to a rolling average?)\n\nWhat sort of numbers would you say offer a reasonably useful/informative model?","pos":35,"type":"cell"}
{"cell_type":"markdown","id":"528af8","input":"We'll start from one of the points we got to in _Sea Levels_, but replicate it directly from numpy.\n\n* Produce a scatter plot of the Fremantle data, along with a line showing the mean.\n  * Set the transparency (alpha value) of the scatter plot to 0.5.\n  * Draw the mean as a yellow line, with a line width of 4.\n  * Include a legend.\n\nYour graph should look like this.\n\n<div>\n<img src=\"mean.png\" width=\"600\">\n</div>\n\n","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"553d58","input":"#### Loading the data\n\nWe also have some more efficient tools now for processing the data, and this time we'll read them straight into numpy arrays.\n\nRecall that last time we stored each column in a list, and combined them into a list of tuples where needed by 'zipping' the lists together.\n\nThis time we will instead store all the relevant columns in a single 2-D array.\n\nWe found from the Case Study that the flags (column 4) didn't provide us with any information, so we are going to store the first 3 columns in an $n \\times 3$ array (that is, some number $n$ rows, and 3 columns).\n\nSince we are only dealing with numerical data this will provide a concise and efficient way of doing it. One disadvantage is that we will no longer have the variables like `dates` and `msls` (mean sea levels) to refer to the data in the different fields, so we will use constants for the column indices instead. (We will see another way to do this in the next case study!)\n\nSo if our data is in an $n \\times 3$ array called, say, `sealevel_data`, we know we can pick out the columns using array selection. For example we could get the date column using `sealevel_data[:,0]` (all rows, column 0).\n\nSo its easier to read and debug, we'll again use a meaningful constants (say DATE for the date column, and another for the mean sea level column, and so on). Then we can instead write `sealevel_data[:,DATE]`. This immediately shows the intent of the code, and makes it easier to read and maintain.\n\nLet's start with the following constants:\n\n","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"594a57","input":"* According to your model, what is the rate of increase (to two decimal places) in sea level?\n\nTo answer this you will need to consider what units the slope is expressed in.\n\n* Print out the rate of increase with the appropriate units.\n\n","pos":23,"type":"cell"}
{"cell_type":"markdown","id":"67039f","input":"#### Second-order model\n\nBased on your earlier work with the line segments, you have probably made some predictions about which way a second-order model will \"bend\".\n\n* Add a second-order model to your plot (using the `order` parameter).\n\nDoes it bend in the direction you expected? Does it bend as much as you anticipated? If not, why do you think this is the case?\n\n","pos":39,"type":"cell"}
{"cell_type":"markdown","id":"67c3c4","input":"","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"69b742","input":"## Modelling the Sea Level Data\n\nWe will begin by examining the sea level data from the Case Study _Will UWA Go Under Water_ (Sea Levels). Recall that this case study used sea level measurements from the National Oceanography Centre's [Permanent Service for Mean Sea Level](https://www.psmsl.org/) (PSMSL). We focussed on the Fremantle station, which had longitudinal data dating back to 1897.","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"71bfb2","input":"### Data acquisition and cleaning\n\nThe data acquisition was covered in the Case Study. We'll use the same source here and once again make use of the data from the Fremantle monitoring station (Station 111). The original data is provided in the file `111.rlrdata.txt` included with this lab.\n\n* Have another look at this data to refresh your memory of the format.\n\nRecall there were some [Data Notes](https://www.psmsl.org/data/obtaining/notes.php) that explained the columns, or *fields*, in more detail.\n\n","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"73ab56","input":"## Regression Models for Rainfall and Temperature\n\nThe general trend in the sea level data, given the length of time over which it was recorded, was fairly easy to recognise with the 'naked eye'. The rainfall data (from the Rainfall lab) and temperature data (from the Heat labs) were not as clear cut.\n\nWe did some quanititative analysis using stepwise linear models based on means.\n\nWe now have the tools to do a more comprehensive quantitative analysis using polynomial models!\n\nUsing a _new notebook_ (it will be faster to not have all the datasets in memory), investigate those data and see what you find!","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"79630d","input":"#### Cleaning the data\n\nThe next step was to remove the dates (months) with missing data. These had the mean sea level recorded as -99999.\n\nTo do this we introduced the idea of masking, and masked each list (dates, msls and missing days) separately.\n\nWith a 2-D array we can mask _all the columns at once_. Remember that \"masking\" is just a form of selection where we use a boolean (or integer) array. In this case, we'll apply that selection to the rows, and take all the columns.\n\n* Use masking/selection (no loops) to create a new array with the lines with missing readings removed. (About 2 lines of code)\n\nAs usual, as a 'sanity check' output the resulting shape. Is it what you expected?\n","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"8b8d47","input":"### Sea level modelling conclusions - scenario planning and risk mitigation\n\n* Given the length of time-series you have available, and all the modelling you've done, what would you suggest is the \"best\" model (or models) for the data?\n\nNote that the best model is subjective - there is not a single right answer.\n\nBecause there is never a single \"right\" model, a sensible approach (which doesn't seem to be taken often enough in today's politics) is to engage in *scenario planning* - looking at a number of reasonable models and their implications, and planning accordingly.\n\nThe more potentially damaging or catastrophic the impacts of a scenario, the more important it is to include \"*worst case models*\" and mitigate against the risks. This is known as *risk mitigation*.\n\n* What are the potential implications of your different sea-level models for the world?","pos":42,"type":"cell"}
{"cell_type":"markdown","id":"974d1e","input":"## Polynomial Models and `scikit-learn`\n\nIn this unit we have made a point of not using 'black box' code from libraries before we understand it well enough that we could write the code ourselves.\n\nWe are going to make one exception with regression on polynomials (as it would take a lot of time to do it in detail). However, as mentioned in the lectures, we know conceptually how we would do it:\n * we know that there will be a number of parameters (or coefficients) based on the order (or degree) of the polynomial function\n * we know that there will be an error function, such as MSE, typically based on least squared error\n * we know that the code will try to choose parameters that minimise that error\n\nSo that we can see how polynomial models work for our data we'll use the machine learning toolkit `scikit-learn` and provide some 'helper functions' so that we can just use it without needing to go into the details.","pos":36,"type":"cell"}
{"cell_type":"markdown","id":"b04d0a","input":"## Challenge\n\nIn the _CensusAge_ lab we implemented a \"spread\" function to \"interpolate\" the age data for age bands above 80. At the end you were asked to think about more realistic ways of spreading the individuals amongst the age categories.\n\nCould any of your modelling techniques be used as a more justifiable way of augmenting the data? Give this a try!","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"d5cd33","input":"#### Q3. Piecewise linear modelling function [1 lab mark]\n\n* Write a function `piecewise_linear(data, segments, quiet=False)` that:\n  * takes a set of data in the format returned by `get_clean_data()`\n  * takes a number of segments, `segments`, into which to split the data (as split by `numpy.array_split()`)\n  * takes a boolean argument `quiet` that can be used to 'turn off' the graphical output, and defaults to False\n  * if `quiet` is False, plots the data points, line of best fit (linear model) for the whole data, and \"segments\" line segments fitting each of the segments of data\n  * returns a \"segments x 2\" array of parameters for the linear 'pieces' where:\n    * the first column contains the slopes for each segment\n    * the second column contains the y-intercepts for each segment\n\nFor example, for 10 segments your output should look like this:\n\n<div>\n<img src=\"segments.png\" width=\"600\">\n</div>\n\n","pos":31,"type":"cell"}
{"cell_type":"markdown","id":"d7a38f","input":"As we saw in the GDP case study, there may be differences between shorter term trends and longer term trends.\n\n* Repeat your linear model plot from above, but this time add an extra black line showing the linear trend for only the *second half of the data*. Be sure to include a legend.\n\nShould there be an odd number of data points, include the midpoint in the second half. Use a linewidth of 4 for all lines.\n\nTip: You may find the `np.ceil()` function useful.\n\n* What is the rate of increase for the second half of the data? How does this compare with the data as a whole?\n\nDoes this suggest the _rate of change_ is increasing or decreasing?\n\n* What are the predicted sea levels for 2020, 2050, 2100 and 3000 using the second half of the data? How do these compare with the whole data?\n\n","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"dd4ffb","input":"Recall that we adopted a data cleaning policy where we rejected *any month with more than 5 missing days*.\n\n* Use masking/selection to create a new array with the months with more than 5 missing days removed. (~2 lines)\n\nAs usual check the shape.","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"f3c7d7","input":"&copy; Cara MacNish, UWA","pos":45,"type":"cell"}
{"id":0,"time":1666845809581,"type":"user"}
{"last_load":1666845808764,"type":"file"}